# -*- coding: utf-8 -*-
"""InitialExperience.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aKIH-pLGyMZDweLAiBn6uoCDdMt0D4fU
"""

!pip install torchcodec

!pip install pydub

"""# Read DataSet"""

from google.colab import drive
drive.mount('/content/drive')

audio_folder = "/content/drive/MyDrive/InitialSample/Initial"
text_xlsx_path = "/content/drive/MyDrive/InitialSample/p3.xlsx"

import pandas as pd
df = pd.read_excel(text_xlsx_path)
print(df.head())

"""## Test"""

from pydub import AudioSegment
import os

file_path = os.path.join(audio_folder, "13159.opus")

try:
    audio = AudioSegment.from_file(file_path)
    print("âœ… Audio loaded successfully!")
    print("Length (ms):", len(audio))
    print("Number of channels:", audio.channels)
    print("Sample rate:", audio.frame_rate)
except Exception as e:
    print("âŒ Failed to read the file:", e)

from IPython.display import Audio

Audio(file_path)

import librosa
import matplotlib.pyplot as plt

y, sr = librosa.load(file_path, sr=16000)

print("âœ… Sample rate:", sr)
print("ğŸ“ Number of samples:", len(y))

# Plot waveform
plt.figure(figsize=(10, 2))
plt.plot(y)
plt.title("Waveform")
plt.show()

"""# Preprocssing

## Audio

### Convert All Audio Files to WAV Format (Mono - 16kHz)
"""

import os

# Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
base_folder = "/content/drive/MyDrive/InitialSample/Initial"

# Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
audio_extensions = (
    ".mp3", ".wav", ".flac", ".ogg", ".aac", ".m4a",
    ".wma", ".alac", ".aiff", ".amr", ".opus"
)

# Ø¹Ø¯Ù‘ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙˆØªÙŠØ© ÙÙŠ ÙƒÙ„ Ù…Ø¬Ù„Ø¯
for root, dirs, files in os.walk(base_folder):
    count = sum(1 for f in files if f.lower().endswith(audio_extensions))
    if count > 0:  # Ø¨Ø³ Ù†Ø·Ø¨Ø¹ Ø¥Ø°Ø§ ÙÙŠÙ‡ Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ©
        print(f"{root} : {count} Ù…Ù„Ù ØµÙˆØªÙŠ")

import os
from pydub import AudioSegment

# Define folders
output_folder = "/content/converted_audio"
os.makedirs(output_folder, exist_ok=True)

# List of supported extensions
valid_extensions = [".opus", ".ogg", ".mp3", ".wav", ".m4a", ".flac",".ogx"]

# Convert all files to .wav (Mono, 16kHz)
for filename in os.listdir(audio_folder):
    name, ext = os.path.splitext(filename)

    if ext.lower() in valid_extensions:
        try:
            file_path = os.path.join(audio_folder, filename)

            audio = AudioSegment.from_file(file_path)
            audio = audio.set_channels(1)
            audio = audio.set_frame_rate(16000)

            output_path = os.path.join(output_folder, f"{name}.wav")
            audio.export(output_path, format="wav")

            print(f"âœ… Converted: {filename} â†’ {name}.wav")
        except Exception as e:
            print(f"âŒ Failed to convert {filename}: {e}")

"""### Clean Audio Files (Trim Silence + Noise Reduction)

"""

!pip install noisereduce

import librosa
import noisereduce as nr
import soundfile as sf
import numpy as np

input_folder = "/content/converted_audio"
output_folder = "/content/processed_audio"
os.makedirs(output_folder, exist_ok=True)

def clean_audio(input_path, output_path):
    try:
        # Load audio with librosa (Mono, 16kHz)
        y, sr = librosa.load(input_path, sr=16000)

        # Noise reduction using first 0.5 sec as noise sample
        noise_sample = y[:int(0.5 * sr)]
        y_denoised = nr.reduce_noise(y=y, y_noise=noise_sample, sr=sr)

        # Trim silence
        y_trimmed, _ = librosa.effects.trim(y_denoised, top_db=20)

        # Save cleaned audio
        sf.write(output_path, y_trimmed, sr)
        print(f"âœ… Cleaned: {os.path.basename(input_path)}")
    except Exception as e:
        print(f"âŒ Failed to clean {input_path}: {e}")

# Apply to all .wav files
for filename in os.listdir(input_folder):
    if filename.endswith(".wav"):
        input_path = os.path.join(input_folder, filename)
        output_path = os.path.join(output_folder, filename)
        clean_audio(input_path, output_path)

"""### Listen to Sample Processed Audio Files

"""

import IPython.display as ipd
import random
import os

processed_folder = "/content/processed_audio"
processed_files = [f for f in os.listdir(processed_folder) if f.endswith(".wav")]

sample_files = random.sample(processed_files, min(20, len(processed_files)))

for i, filename in enumerate(sample_files):
    print(f"ğŸ”Š Sample {i+1}: {filename}")
    display(ipd.Audio(os.path.join(processed_folder, filename)))

"""## Text"""

text_column = "IrbidDial"

import re
import string


def normalize_arabic(text):
    text = str(text)
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)
    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ùˆ", text)
    text = re.sub("Ø¦", "ÙŠ", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("[^Ø¡-ÙŠ\s]", "", text)
    text = re.sub("\s+", " ", text).strip()
    return text

# Apply normalization function to the text column
df[text_column] = df[text_column].astype(str).apply(normalize_arabic)

# Remove Arabic and English numbers (just in case)
df[text_column] = df[text_column].apply(lambda text: re.sub(r"[0-9Ù -Ù©]+", "", text))

'''# Tokenize the text (split into words)
df["tokens"] = df[text_column].apply(lambda text: text.split())
df["tokens"].head(56)'''

"""## Connect Speech with Irbid Dialect Text

"""

df["audio_path"] = df["sentID.BTEC"].apply(
    lambda x: f"/content/processed_audio/{int(x)}.wav" if pd.notna(x) else None
)

df["audio_path"]

import os

df["audio_exists"] = df["audio_path"].apply(
    lambda path: os.path.exists(path) if pd.notna(path) else False
)

df = df[df["audio_exists"] == True].reset_index(drop=True)

import IPython.display as ipd

num_samples = 10

for i in range(num_samples):
    row = df.iloc[i]
    text = row["IrbidDial"]
    audio = row["audio_path"]

    print(f"\nğŸ“ Text {i+1}: {text}")
    print(f"ğŸ”Š Audio {i+1}: {os.path.basename(audio)}")
    display(ipd.Audio(audio))

"""# Wave2Vec"""

!pip install transformers datasets evaluate jiwer

"""###  Data Splitting"""

# Data Splitting

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)

print(f"Total samples: {len(df)}")
print(f"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

"""###Create Dataset + Filter Missing Audio"""

# Create Dataset + Filter Missing Audio

from datasets import Dataset, Audio
import os

train_dataset = Dataset.from_pandas(train_df[["audio_path", "IrbidDial"]])
val_dataset = Dataset.from_pandas(val_df[["audio_path", "IrbidDial"]])
test_dataset = Dataset.from_pandas(test_df[["audio_path", "IrbidDial"]])

train_dataset = train_dataset.filter(lambda x: os.path.exists(x["audio_path"]))
val_dataset = val_dataset.filter(lambda x: os.path.exists(x["audio_path"]))
test_dataset = test_dataset.filter(lambda x: os.path.exists(x["audio_path"]))

train_dataset = train_dataset.cast_column("audio_path", Audio())
val_dataset = val_dataset.cast_column("audio_path", Audio())
test_dataset = test_dataset.cast_column("audio_path", Audio())

"""### Load Pretrained Model & Processor"""

# Load Pretrained Model & Processor

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

processor = Wav2Vec2Processor.from_pretrained("jonatasgrosman/wav2vec2-large-xlsr-53-arabic")

model = Wav2Vec2ForCTC.from_pretrained(
    "jonatasgrosman/wav2vec2-large-xlsr-53-arabic",
    attention_dropout=0.2,
    hidden_dropout=0.2,
    feat_proj_dropout=0.2,
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
    mask_time_prob=0.1,
    mask_time_length=10
    mask_feature_prob=0.05,
    mask_feature_length=64,
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
)
model.freeze_feature_encoder()
print("âœ… Model and processor loaded successfully!")

"""### Preprocess Audio/Text"""

# Preprocess Audio/Text

import torchcodec

def prepare_dataset_no_pad(batch):
    audio = batch["audio_path"]
    batch["input_values"] = audio["array"]
    batch["labels"] = processor.tokenizer(batch["IrbidDial"]).input_ids
    return batch

train_dataset = train_dataset.map(prepare_dataset_no_pad)
val_dataset = val_dataset.map(prepare_dataset_no_pad)
test_dataset = test_dataset.map(prepare_dataset_no_pad)

"""### Data Collator"""

# Data Collator

from dataclasses import dataclass
from typing import Dict, List, Union
import torch

@dataclass
class DataCollatorCTCWithPadding:
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_values = [{"input_values": f["input_values"]} for f in features]
        labels = [{"input_ids": f["labels"]} for f in features]

        batch = self.processor.feature_extractor.pad(
            input_values,
            padding=self.padding,
            return_tensors="pt",
        )
        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": [f["labels"] for f in features]},
            padding=self.padding,
            return_tensors="pt",
        )
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        batch["labels"] = labels
        return batch

data_collator = DataCollatorCTCWithPadding(processor=processor)

"""### Compute Metrics (WER & CER)"""

# Compute Metrics (WER)

import evaluate
import numpy as np

wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

def compute_metrics(pred):
    pred_ids = np.argmax(pred.predictions, axis=-1)
    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)

    label_ids = pred.label_ids
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id
    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer, "cer": cer}

"""### Training Arguments"""

# Training Arguments

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./wav2vec2-irbid",
    eval_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=1,
    learning_rate=5e-5,
    warmup_ratio=0.2,
    num_train_epochs=20,
    fp16=True,
    save_strategy="epoch",
    logging_steps=10,
    eval_steps=50,
    save_total_limit=2,
    report_to="none",
    optim="adamw_torch",
    lr_scheduler_type="cosine",
    weight_decay=0.01
)

"""### Trainer + Train


"""

# Trainer + Train
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=processor,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

trainer.train(resume_from_checkpoint=False)

"""### Evaluate on Test Set



"""

# Evaluate on Test Set

results = trainer.evaluate(test_dataset)
print(f"WER Test: {results['eval_wer']:.2f}")
print(f"CER Test: {results['eval_cer']:.2f}")

"""### Visualization"""

import pandas as pd
import matplotlib.pyplot as plt
import os

# Load the training logs
log_history = trainer.state.log_history

# Convert to DataFrame
log_df = pd.DataFrame(log_history)

# Filter rows with loss and eval_loss and corresponding epoch
train_loss_df = log_df[log_df["loss"].notnull()][["epoch", "loss"]]
eval_loss_df = log_df[log_df["eval_loss"].notnull()][["epoch", "eval_loss"]]

# Plot
brown_color = "#7A6149"

plt.figure(figsize=(10, 5))
plt.plot(train_loss_df["epoch"], train_loss_df["loss"], label="Training Loss", color=brown_color)
plt.plot(eval_loss_df["epoch"], eval_loss_df["eval_loss"], label="Validation Loss", color="#A17C5B")  # lighter shade
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs. Validation Loss per Epoch")
plt.legend()
plt.grid(True)
plt.show()

trainer.state.log_history

import pandas as pd
import matplotlib.pyplot as plt

# Extract training logs
log_df = pd.DataFrame(trainer.state.log_history)

wer_df = log_df[log_df["eval_wer"].notnull()][["epoch", "eval_wer"]]

# Plot WER per epoch
plt.figure(figsize=(10, 5))
plt.plot(wer_df["epoch"], wer_df["eval_wer"], marker='o', linestyle='-', color='#A17C5B', label="WER")
plt.xlabel("Epoch")
plt.ylabel("WER")
plt.title("WER per Epoch on Validation Set")
plt.grid(True)
plt.legend()
plt.xticks(wer_df["epoch"])
plt.ylim(0, 1)

# Highlight best epoch
best_index = wer_df["eval_wer"].idxmin()
best_epoch = wer_df.loc[best_index, "epoch"]
best_wer = wer_df.loc[best_index, "eval_wer"]
plt.scatter(best_epoch, best_wer, color='red', zorder=5, label=f"Best WER ({best_wer:.2f})")
plt.legend()

plt.show()

"""### Predict Batch of Files"""

import librosa
import torch
from IPython.display import Audio
import pandas as pd

def predict_and_compare_by_filename(file_name, df):

    audio_path = f"/content/processed_audio/{file_name}"

    audio, sr = librosa.load(audio_path, sr=16000)
    print(f"ğŸ§ Length :{len(audio)/sr:.2f} Second")

    inputs = processor(audio, sampling_rate=sr, return_tensors="pt", padding=True)
    inputs = {key: val.to(model.device) for key, val in inputs.items()}

    with torch.no_grad():
        logits = model(**inputs).logits
    pred_ids = torch.argmax(logits, dim=-1)
    predicted_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]

    print("\n Predicted: ")
    print(predicted_text)

    match = df[df["audio_path"].str.contains(file_name, case=False)]
    if len(match) > 0:
        ground_truth = match["IrbidDial"].values[0]
        print("\n Ground truth:  ")
        print(ground_truth)
    else:
        print("\nâš ï¸")

    return Audio(audio_path)

predict_and_compare_by_filename("13291.wav", df)

import librosa
import torch
from IPython.display import Audio, display

def predict_and_compare_batch(df, max_samples=10):
    samples_shown = 0

    for i in range(len(df)):
        file_path = df.iloc[i]["audio_path"]
        if not isinstance(file_path, str) or not file_path.endswith(".wav"):
            continue

        try:

            audio, sr = librosa.load(file_path, sr=16000)

            inputs = processor(audio, sampling_rate=sr, return_tensors="pt", padding=True)
            inputs = {key: val.to(model.device) for key, val in inputs.items()}

            with torch.no_grad():
                logits = model(**inputs).logits
            pred_ids = torch.argmax(logits, dim=-1)
            predicted_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]

            ground_truth = df.iloc[i]["IrbidDial"]

            print(f"\n=================== Sample {samples_shown+1} ===================")
            print("Ground truth: ")
            print(ground_truth)
            print("\nPredicted: ")
            print(predicted_text)
            print("ğŸ§")
            display(Audio(file_path))

            samples_shown += 1
            if samples_shown >= max_samples:
                break

        except Exception as e:
            print(f"âš ï¸{file_path}: {e}")

predict_and_compare_batch(df, max_samples=300)

# Ø¨Ø¹Ø¯ Ø¹Ù…Ù„ zipØŒ Ø§Ù†Ø³Ø®Ù‡ Ø¥Ù„Ù‰ Drive
!cp /content/content.zip /content/drive/MyDrive/
print("ØªÙ… Ù†Ø³Ø® all_content.zip Ø¥Ù„Ù‰ MyDrive â€” Ø§ÙØªØ­ Google Drive Ù„ØªØ­Ù…ÙŠÙ„Ù‡.")

